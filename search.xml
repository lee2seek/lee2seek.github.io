<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Refactor-improving the Design of Existing Code]]></title>
    <url>%2FRefactor-improving-the-Design-of-Existing-Code.html</url>
    <content type="text"><![CDATA[前言Refactor-improving the Design of Existing Code(重构-改善既有代码的设计),这是Martin Fowler的一本书.主要是针对Java语言.书已经出版多年了,买这本书也有一年多了,但是一直只看了第一章.最近因为工作不是很忙,就拾起来读了.这里主要是记录下读这本书的一些感想以及收获.读这本书有个收获,重构不单单只是修改函数名称,重命名变量名称那么简单.比如进行类的分解,将合适的函数放置在合适的类中.而这些都需要我们在平常的编程活动中去实践.什么时候进行重构呢?当遇到几个方面时,可以考虑进行重构.发现当为函数添加新功能时,并不能很好的添加进去,这时候我们可以进行重构;当写完功能时,我们也可以考虑重构.总之,重构并不是开发过程中必须要通过的一个过程,就像编译-链接等.你可以随时进行重构. 重构我们在平时编程的时候,不可能一次性的就把问题或者程序写的完美无缺,除非我们已经非常熟知某个问题或者某个领域,即使非常熟悉,可能还有我们没有发现的提升之处.那么当我们的程序写的不完美的时候,我们怎么办呢?这时候就可以采用重构的方法来小步快跑的提升我们程序的易读性,优化程序的结构.而这本书给我们提供了如何进行重构的一系列方法.整个开发流程:开发—-&gt; 测试 —-&gt; 重构 —-&gt; 测试 测试在我们重构之前,一定要有合适的测试.在Java中,我们经常用的就是Junit和TestNG了.其中Junit主要是用来做单元测试,而TestNG主要是功能测试或者集成测试.如果我们是开发人员,可能使用Junit比较多,不过TestNG一般也会用到.在我们重构完成之后,一定要跑测试用例,跑通了所有的测试用例,这项重构才算是完成. 重构方法提取方法在重构中有一个很核心的动作就是将代码提取为一个单独的方法.这种方式其实也杜绝了重复代码的出现,能够在我们修改代码的时候只需要修改一处就可以.并且能够在多个地方使用 将注释提取为方法理想的程序就是代码完全能够表达自己,当我们在程序中看到注释或者添加注释的时候,我们可以先思考下能否将注释下的代码提取为一个方法,并且方法的函数名称能够体现注释的内容,方法名称要体现程序做了什么而不是怎么做. 明确类的职责类的职责不宜过于多,不宜过于复杂.如果看到一个类承担的职责很多,我们可以考虑是否可以将此类拆分,将不属于其应该承担的责任提取到一个新的类中.然后在源类中通过引用来使用新类中的字段或方法 用查询来代替变量就是在我们使用变量时,我们应该采用计算变量的方法来替代此变量.其实对于这一做法,我是抱有怀疑态度的,因为这造成了函数运行多次.会造成性能下降,但是书中说性能优化属于优化阶段的工作,而且这样重构会为优化阶段带来很好的铺垫.这个我觉得还是主要看平时我们的工作中需要具体情况具体分析.不能尽信书,什么东西都需要我们自己的独立思考 条件表达式条件表达式的两种形式:1.所有分支都属于正常分支;2.只有一种是正常行为,其他都是不正常行为.针对情况1,建议使用if…else结构;针对情况2,使用if进行判断,然后直接返回结果.这样子处理能够提高程序清晰度 封装集合当我们在类中有个字段是集合时,我们的返回函数不应该直接返回集合自身,而是应该返回集合的只读副本.另外,不应该为这整个集合提供一个设值函数,应该提供为集合添加,删除元素的函数.]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notebook-jupyter-cookbook]]></title>
    <url>%2Fnotebook-jupyter-cookbook.html</url>
    <content type="text"><![CDATA[显示docstring在编辑界面按下shift+tab组合键可以显示模块或函数的docstring,连续按下两次可以显示更加详细的信息]]></content>
      <tags>
        <tag>notebook</tag>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n Convolutional Neural Networks for Visual Recognition]]></title>
    <url>%2FCS231n-Convolutional-Neural-Networks-for-Visual-Recognition.html</url>
    <content type="text"><![CDATA[这是一篇翻译文章,但是也不全是,主要是读了文章之后,用自己的话将其复述出来来源: CS231n Convolutional Neural Networks for Visual Recognition CNN卷积神经网络跟一般的神经网络是非常相似的.它们都由神经元组成,并且这些神经元上都有需要学习的权重和偏置项.每个神经元接收输入,执行点积,然后可选择的将输出进行非线性运算.整个网络表示了一个可微的得分函数:即从原始的图像像素到对应的类.两者都有一个损失函数(例如:SVM/Softmax),并且在一般神经网络中用到的技术也能应用到CNN中.两者的区别在哪里呢?ConvNet网络假设输入是图像,所以我们能够将某些属性编码到网络结构中.这使得前馈函数更有效率并且能极大的减少网络中的参数. CNN结构概览回顾:在一般的神经网络中,神经网络接收一个输入(一个向量),然后经过一系列的隐藏层进行转换.每个隐藏层是由一组神经元组成,每个神经元都与前一层的所有神经元相连,属于同一层的神经元完全独立并且不共享任何连接.最后一层是输出层,在分类问题中,它代表了类的得分.一般的神经网络不能很好的扩展到完整的图像.在CIFAR-10中,一副图像的大小是32$\times$32$\times$3,所以隐藏层中的一个神经元上将会有32$\times$32$\times$3=3072个权重.参数的数量仍然可以接受,但是全连接结构不能扩展到更大的图像上了.例如,如果图像的大小为200$\times$200$\times$3,那么一个神经元将需要200$\times$200$\times$3=120000个权重.可见,全连接神经网络需要的参数将会非常多.这会导致过拟合.卷积神经网络充分利用了输入是图像的事实,并且用一个更加合理的方式约束了神经网络的结构.特别的,与一般的神经网络不同,卷积神经网络各层由三维排列的神经元组成:宽度,高度,深度(注意,这里的深度指的是激活的第三维,而不是整个神经网络的深度).例如,在CIFAR-10中的输入图像的维度是32$\times$32$\times$3.我们很快会看到,当前层的神经元仅仅连接前一层中的部分神经元.此外,对于CIFAR-10最终的输出层的维度是1$\times$1$\times$10,因为ConvNet最后会将整个图像转换为一个类得分向量,下面是可视化:上图表示一个三层的神经网络,下图表示卷积神经网络. A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters. ConvNet 层就像我们上面描述的,一个简单的ConvNet是一系列层组成,ConvNet的每层通过一个可微的函数将输入转化到输出.ConvNet主要由卷积层,池化层和全连接层组成.下面是一个针对CIFAR-10的简单例子: INPUT(32$\times$32$\times$3):输入是原始图像的像素,一副图像的宽是32,长是32,并且有三个颜色通道R,G,B 卷积层:当前层的神经元只连接前一层的部分神经元.如果我们使用12个过滤器,则经过卷积层后的维度是[32$\times$32$\times$12] RELU:对于输入采用ReLu激活函数,并不会改变输入维度,如果前一个维度是[32$\times$32$\times$12],则经过ReLu之后仍然是[32$\times$32$\times$12] POOL:池化层会在空间维度上执行下采样,这会导致维度变化,[16$\times$16$\times$12],但是注意最后一维没有改变 FC(全连接层):这是一个全连接的结构,最后的输出是[1$\times$1$\times$10].卷积神经网络就是将原始的像素图像经过一层一层的计算,最后得到最终的分类得分.注意有些层需要参数,而有些层不需要参数.CONV/FC层不仅仅对于输入进行激活,同时需要将权重和偏置作用在输入上.而RELU/POOL只是一个固定的函数,并没有参数.总结: 卷积神经网络就是一系列层组成,将输入图像体积转换为输出体积(体积表面了维度) 卷积神经网路包括完全不同的层(e.g. CONV/FC/RELU/POOl) 每一层通过一个可微的函数将3D volume的输入转换为3D volume的输出 有些层需要参数,有些不需要(e.g. CONV/FC 需要, RELU/POOlL不需要) 有些层需要额外的超参数,有些不需要(e.g. CONV/FC/POOL需要,RELU不需要) 因为无法很难画出3D的部分,所以这里每一层只展示了深度部分的一片.最后给出了得分最高的五个标签.这里展示的是一个很小的 VGG网络.查看详细展示卷积层卷积层是卷积神经网络的核心部分,并且涉及了大量的计算.卷积层使用过滤器对于原图像进行卷积,过滤器每次只能针对整副图像的一部分进行计算,所以我们需要移动过滤器,遍历整个图像.这里过滤器的大小又叫做神经元的接收域.Example 1:假设输入为[32$\times$32$\times$3],过滤器大小为5$\times$5,那么卷积层中的某个神经元需要的参数为5$\times$5$\times$3 + 1=76.为什么需要这么多的参数呢?针对颜色通道R,当前过滤器对应的局部区域的点是5$\times$5=25个,有三个通道,所有总的参数为75,另外再加一个偏置项,所有一个神经元总共需要76个参数.注意这里的深度为3,这是因为输入的深度是3.Example 2:假设输入为[16$\times$16$\times$20],过滤器大小为3$\times$3,那么卷积层中每个神经元都需要3*3*20 + 1=180个参数. 第一张图展示了卷积层,可以看到一个神经元连接了原图像的局部区域,但是连接了所有的深度(这是是三个颜色通道).这里展示了五个神经元,这五个神经元都连接到了图像的同一个局域上.第二张图展示了神经元的计算.Spatial arrangement 前面我们仅仅讨论了卷积层中的每个神经元如何连接到前一层,我们还没有讨论在卷积层的输出中有多少个神经元.深度,步长,0值填充这些超参数控制着卷积层的输出的大小 卷积层输出的深度等于过滤器的数量,而每个过滤器就是去寻找输入到卷积层数据的不同之处.如果输入是原始图像,那么过滤器就是去寻找不同方向的边,颜色等. 步长是过滤器移动的长度,一般常用的是1和2 0值填充就是在卷积层的输入的边界上填充0值,使用0值填充使得我们能够控制经过卷积层之后的空间大小.下面的公式可以用来计算卷积层输出的空间大小(W - F + 2P)/S + 1其中W(输入数据的大小),F(卷积层神经元的接收域大小),S(步长),P(零值填充的宽度).例如输入为7$\times$7,过滤器为3$\times$3,步长为1,不填充,则得出输出为5$\times$5.当步长变为2时,那么输出变为3$\times$3. 这里的输入只有一个x轴,输入为[1,2,-1,1,-3],过滤器大小为3,采用零值填充.所以W=5,F=3,P=1.图片最右侧是过滤器的权重,偏置为0.左图:步长为1,最后得到的输出的尺寸为5;右图:步长为2,最后得到的输出尺寸为3.所有黄色的神经元共享相同的参数 Use of zero-padding.当步长为1(即S=1),设置$P=(F-1)/2$,这样卷积层的输入跟输出将会有相同大小的空间.Constraints on strides. 池化层Normalization Layer全连接层全连接层转变为卷积层ConvNet 结构层模式层大小模式常用CNN计算考虑参考]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建github博客多设备更新]]></title>
    <url>%2Fhexo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2%E5%A4%9A%E8%AE%BE%E5%A4%87%E6%9B%B4%E6%96%B0.html</url>
    <content type="text"><![CDATA[前段时间由于电脑重新安装了系统,导致自己的博客文件丢失,而github上面的只有发布后的文件,没有博客的源文件。想要恢复到源文件至今还没有找到解决方法,好在原来的博客内容不多,丢失了也无所谓了。但是以后要是换电脑了,这种问题怎么解决呢?今天介绍一个解决方案。其实方法很简单,首先应该有两个分支,一个分支用来保存发布后的内容,一个分支用来保存源文件.这里用master分支保存发布后的内容,hexo分支保存源文件内容. 第一步设备A上搭建github博客 第二步在博客根目录下,打开_config.yml,添加如下内容1234deploy: type: git repository: git@github.com:youname/youname.github.io.git branch: master 可以看到现在已经有一个master分支,但是这时候个人博客还不是一个git目录 提交代码12// hexo编译源文件，生成静态文件，也可以分开执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d hexo clean: 清空博客缓存hexo g(hexo generator 的简写):生成静态文件hexo d(hexo deploy的简写): 部署文件,这条命令会使用第一步中的配置信息进行部署 现在打开yourname.github.io就能够看到你的博客了 第三步因为我用的是next主题(其他主题做相似处理).删除next文件下的.git文件夹,这是因为我们在要我们的博客下创建.git,如果子目录下也有.git,会有问题.然后执行以下命令12345678910// git初始化git init// 新建分支并切换到新建的分支git checkout -b 分支名// 添加所有本地文件到gitgit add .// git提交git commit -m &quot;提交说明&quot;// 文件推送到hexo分支git push origin hexo 以后操作都是在hexo分支中,当我们修改了我们的博客内容时,先执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ,这个命令用来将本地博客发布到github上然后在将本地内容提交到hexo分支中 第四步假设我们需要在电脑B上搭建我们之前的博客内容.我们需要拉取我们的hexo分支,因为hexo分支是源文件,master可以不用拉取123456// 克隆分支到本地git clone -b hexo https://github.com/用户名/仓库名.git// 进入博客文件夹cd youname.github.io// 安装依赖npm install 第五步电脑B上编辑博客内容,静态文件提交到master分支,源文件提交到hexo分支1234567891011//博文提交到master上面。hexo clean &amp;&amp; hexo g &amp;&amp; hexo d//源文件提交到hexo分支上面。// 添加源文件git add .// git提交git commit -m &quot;&quot;// 先拉原来Github分支上的源文件到本地，进行合并git pull origin hexo// 比较解决前后版本冲突后，push源文件到Github的分支git push origin hexo 第六步在电脑A上可以同步hexo分支,开始更新博客 注意: 以后操作都是在hexo分支中,当我们修改了我们的博客内容时,先执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ,这个命令用来将本地博客发布到github上然后在将本地内容提交到hexo分支中]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
