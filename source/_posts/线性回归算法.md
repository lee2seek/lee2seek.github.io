---
title: 线性回归算法
date: 2018-09-05 23:41:13
categories:
- 机器学习
tags:
- 线性回归算法
- 局部加权回归算法
- 岭回归算法
- lasso回归算法
copyright:
top:
type:
mathjax: true
---
# 线性回归算法

回归算法一般是针对预测是连续的情况下，对于预测值是离散的，采用的算法是分类算法。线性回归算法包括很多种变形，这里提到的线性回归算法是其中的几种典型算法。在实际应用中，我们采用线性算法可以预测商品的价格，预测房子的房价等等，虽然线性回归算法比较简单，但是在实际中还是有很多的使用的。

> 在机器学习中，我们要紧盯三件事情。第一，算法的损失函数；第二，采用什么求值算法求损失函数的最小值；第三，算法的评价指标

## 一般线性回归算法

一般的线性回归又叫做最小二乘算法，最小二乘是因为算法的损失函数是最小二乘的，损失函数如下：
$$J(\theta)={\frac 12}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \tag {1.1}$$
其中$h_\theta(x^{(i)})$是算法针对数据的预测值，而$y^{(i)}$是数据的真实值，$m$表示训练数据的条数，而${\frac 12}$是为了此公式求导方便而加入的。而$\theta$是算法的参数，在这里就是线性回归的权重值。通过此公式我们可以得到，线性回归算法的损失函数就是针对每个样本计算预测值和真实值得差，然后将差求平方，之后将全体样本的差平方相加即得到损失函数。
针对损失函数，我们有两种算法可以求取损失函数的最小值。

### 梯度下降算法

梯度下降算法的一般形式：
$$\theta_j: = \theta_j - \alpha{\frac \partial  {\partial\theta_j}}J(\theta) \tag {1.2}$$
这里写的是梯度下降算法的标量形式。这个公式描述了梯度下降算法是如何更新算法的参数的，其中$\alpha$是参数的更新步长。可以看到这里的关键是如何求取损失函数关于参数j的偏导数。
将损失函数带入到梯度下降算法,即公式$(1.2)$中，并且求导，可以得到下式：
$$\theta_j:= \theta_j + \alpha{\sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}} \tag {1.3}$$
我们重复的使用公式$(1.3)$直到达到收敛条件，即可以求得线性回归算法中的参数值。
从公式中，可以看到用真实值和预测值之间的差值来更新参数值，这种方式或者思想在很多的机器学习算法中可以看到，包括深度学习的后向传播算法。同时，可以看到每一次的迭代都要使用整个数据集。这种方式叫做批量梯度下降算法。还有一种方式可以求取$\theta$值，叫做随机梯度下降算法，算法如下：
{% asset_img 随机梯度下降.PNG 随机梯度下降算法 %}
从中我们可以看到，随机梯度下降算法每次采用一个训练样本取更新所有的参数值，注意那里的(for every j)。当训练样本很多时，相比于批量梯度下降算法，随机梯度下降算法能够更快的更新算法的参数值，并且能够更快的逼近损失函数的最小值。

### 代数解

我们将损失函数用向量表示，如下所示：
$$J(\theta) = {\frac 12}(X\theta - \vec{y})^T(X\theta - \vec{y}) \tag{1.4}$$
公式中的$X$表示训练数据的矩阵。因为损失函数是凸二次函数，所以只有一个最小值，所以导数为0的点就是损失函数的最小值。
具体的推导过程如下(主要利用了矩阵的迹运算)：
{% asset_img 损失函数的导数.PNG 推导过程%}
令导数等于0，从而得到：
$$\theta = (X^TX)^{(-1)}X^T\vec{y} \tag{1.5}$$

### 回归模型的概率解释

大家想过没有，为什么在线性回归模型里面选择最小二乘作为损失函数？接下来从概率的角度来解释选择最小二乘作为损失函数的原因。
首先，假设目标变量和输入数据存入如下的关系：
$$y^(i) = \theta^Tx^{(i)} + \epsilon^{(i)} \tag{1.6}$$
这里的$\epsilon^{(i)}$是误差项，包括模型未考虑到影响目标变量的因素和随机噪声。
接下来假设，误差项相互独立同分布，并且服从高斯分布(即正态分布)
> 为什么要假设误差项服从高斯分布? 第一是因为采用高斯分布，在数学上处理会比较简单；第二是因为根据中心极限定理，独立的随机变量的和，其总的影响接近高斯分布

误差项的概率密度函数为：
$$p(\epsilon^{(i)}) = {\frac 1{\sqrt{2\pi}}}exp(-{\frac {(\epsilon^{(i)})^2} 2\sigma^2}) \tag {1.7}$$
根据公式$(1.6)$和公式$(1.7)$,我们可以得出如下结论：
{% asset_img 概率分布.PNG 概率分布 %}
在公式中，$\theta$不是随机变量，而是实际存在的值，虽然我们不知道真实值是多少。$p(y^{(i)}|x^{(i)};\theta)$的含义是给定$x^{(i)}$,参数设定为$\theta$时,$$y^{(i)}$$的概率密度。注意公式中用的分号。
数据的概率是由$p(\vec{y} | X;\theta)$给出的，而总的概率可以看成是在固定$\theta$时，关于$\vec{y}$的函数。换个角度，我们想要将这个函数明确的看成是关于$\theta$的函数，所以我们将其称作似然函数，从而我们得到关于$\theta$的似然函数：
> 似然(likelihood)和概率(probability)实际上是一个东西，但是似然函数是对参数$\theta$定义的，为了加以区分，使用了似然这一术语。我们可以说参数的似然，数据的概率，但不能说数据的似然，参数的概率。

$$L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;\theta)$$
极大似然估计就是选择$\theta$,使参数的似然函数最大化，也就是选择参数使得已有样本的出现概率最大。
因为$L(\theta)$是严格单调递增的，并且对数函数也是递增的，所以取对数，得到的$\theta$跟不取对象是一样的。
{% asset_img 似然函数.PNG 似然函数 %}
对数似然函数$\cal l(\theta)$:
{% asset_img 对数似然函数.PNG 对数似然函数 %}
最大化似然函数，就是最大化对数似然函数，即最小化
$${\frac 12}{\sum\limits_{i=1}^{m}}(y^{(i)} - \theta^Tx^{(i)})^2$$
这个式子刚好是线性回归算法中采用的损失函数。总结一下，最小二乘回归模型刚好就是在假设了误差独立同服从正态分布后，得到的最大似然估计。同时注意到，正态分布中的方差$\sigma^2$的取值对模型并没有影响。

## 局部加权线性回归算法

## 岭回归算法

{% asset_img 岭回归.png lambda值与权重的关系 %}

## lasso回归算法

## 总结